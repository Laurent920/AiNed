{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-based training of NNs in Jax\n",
    "\n",
    "In this notebook, we will explore how to write event based software to do gradient-based learning with neural networks (NN) in [jax](https://github.com/google/jax/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using jax 0.5.1\n",
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit\n",
    "from jax.scipy.special import logsumexp\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import dataclasses\n",
    "from typing import Generic, Any, Union, TypeVar, Tuple\n",
    "import tree_math\n",
    "import numpy as np\n",
    "\n",
    "from jax import config \n",
    "config.update(\"jax_debug_nans\", True)\n",
    "print(\"Using jax\", jax.__version__)\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]]\n",
      "[[4 2 3 1 0]\n",
      " [3 0 4 2 1]]\n",
      "[[1.0000000e+00 2.0000000e+00 3.0000000e+00 4.0000000e+00 5.0000000e+00]\n",
      " [3.2020281e-03 4.8870957e-03 4.9999999e-03 5.1129041e-03 6.7979717e-03]]\n",
      "{CpuDevice(id=0)}\n",
      "SingleDeviceSharding(device=CpuDevice(id=0), memory_kind=unpinned_host)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                            CPU 0                             </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "<span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #393b79\">                                                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                            \u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121mCPU 0\u001b[0m\u001b[38;2;255;255;255;48;2;57;59;121m                             \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n",
       "\u001b[38;2;255;255;255;48;2;57;59;121m                                                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = jnp.array([[5, 4, 2, 3, 1],\n",
    "              [4.8870957e-03, 6.7979717e-03, 5.1129041e-03, 3.2020281e-03, 4.9999999e-03]])\n",
    "\n",
    "print(jnp.arange(a.shape[0])[:, None])\n",
    "sort_idx = jnp.argsort(a, axis=-1)\n",
    "print(sort_idx)\n",
    "input = a[jnp.arange(a.shape[0])[:, None], sort_idx]\n",
    "print(input)\n",
    "\n",
    "print(input.devices())\n",
    "print(input.sharding)\n",
    "jax.debug.visualize_array_sharding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "@tree_math.struct\n",
    "class Neuron:\n",
    "    value: jnp.ndarray\n",
    "    \n",
    "\n",
    "@dataclasses.dataclass\n",
    "@tree_math.struct\n",
    "class InputQueue:\n",
    "    neuron: Neuron\n",
    "    head: int = 0\n",
    "\n",
    "    @property\n",
    "    def is_empty(self) -> bool:\n",
    "        return self.head == len(self.spikes.time)\n",
    "\n",
    "    def peek(self) -> Neuron:\n",
    "        return self.spikes[self.head]\n",
    "\n",
    "    def pop(self) -> Neuron:\n",
    "        spike = self.spikes[self.head]\n",
    "        self.head += 1\n",
    "        return spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512\n",
      "512 512\n",
      "10 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "  w_key, b_key = jax.random.split(key)\n",
    "  return scale * jax.random.normal(w_key, (n, m)), scale * jax.random.normal(b_key, (n,))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "  keys = jax.random.split(key, len(sizes))\n",
    "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "params = init_network_params(layer_sizes, jax.random.key(0))\n",
    "[print(len(w), len(b)) for w, b in params[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "def predict(params, image):\n",
    "  # per-example predictions\n",
    "  activations = image\n",
    "  for w, b in params[:-1]:\n",
    "    outputs = jnp.dot(w, activations) + b\n",
    "    activations = relu(outputs)\n",
    "    \n",
    "  final_w, final_b = params[-1]\n",
    "  logits = jnp.dot(final_w, activations) + final_b\n",
    "  return logits - logsumexp(logits)\n",
    "\n",
    "# This works on single examples\n",
    "random_flattened_image = jax.random.normal(jax.random.key(1), (28 * 28,))\n",
    "preds = predict(params, random_flattened_image)\n",
    "print(preds.shape)\n",
    "\n",
    "# Make a batched version of the `predict` function\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "# `batched_predict` has the same call signature as `predict`\n",
    "random_flattened_images = jax.random.normal(jax.random.key(1), (10, 28 * 28))\n",
    "batched_preds = batched_predict(params, random_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def accuracy(params, images, targets):\n",
    "  target_class = jnp.argmax(targets, axis=1)\n",
    "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "  return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "  preds = batched_predict(params, images)\n",
    "  return -jnp.mean(preds * targets)\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "  grads = grad(loss)(params, x, y)\n",
    "  return [(w - step_size * dw, b - step_size * db)\n",
    "          for (w, b), (dw, db) in zip(params, grads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\me\\desktop\\thesis\\ained\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "  def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "    super(self.__class__, self).__init__(dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        sampler=sampler,\n",
    "        batch_sampler=batch_sampler,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=numpy_collate,\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=drop_last,\n",
    "        timeout=timeout,\n",
    "        worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "  def __call__(self, pic):\n",
    "    return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "# Define our dataset, using torch datasets\n",
    "mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me\\Desktop\\Thesis\\AiNed\\venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\me\\AppData\\Local\\Temp\\ipykernel_9400\\1619607889.py:2: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\n",
      "c:\\Users\\me\\Desktop\\Thesis\\AiNed\\venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Users\\me\\AppData\\Local\\Temp\\ipykernel_9400\\1619607889.py:3: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
      "c:\\Users\\me\\Desktop\\Thesis\\AiNed\\venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:81: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "c:\\Users\\me\\Desktop\\Thesis\\AiNed\\venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:71: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "C:\\Users\\me\\AppData\\Local\\Temp\\ipykernel_9400\\1619607889.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)\n"
     ]
    }
   ],
   "source": [
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)\n",
    "train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
    "\n",
    "# Get full test dataset\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)\n",
    "test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)\n",
    "test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 in 3.40 sec\n",
      "Training set accuracy 0.9134333729743958\n",
      "Test set accuracy 0.9165999889373779\n",
      "Epoch 1 in 3.18 sec\n",
      "Training set accuracy 0.9359000325202942\n",
      "Test set accuracy 0.9353999495506287\n",
      "Epoch 2 in 3.24 sec\n",
      "Training set accuracy 0.9474666714668274\n",
      "Test set accuracy 0.9460999965667725\n",
      "Epoch 3 in 3.17 sec\n",
      "Training set accuracy 0.9559000134468079\n",
      "Test set accuracy 0.9527999758720398\n",
      "Epoch 4 in 3.24 sec\n",
      "Training set accuracy 0.962066650390625\n",
      "Test set accuracy 0.9575999975204468\n",
      "Epoch 5 in 3.32 sec\n",
      "Training set accuracy 0.9663166999816895\n",
      "Test set accuracy 0.9606999754905701\n",
      "Epoch 6 in 3.06 sec\n",
      "Training set accuracy 0.9700333476066589\n",
      "Test set accuracy 0.9633999466896057\n",
      "Epoch 7 in 3.03 sec\n",
      "Training set accuracy 0.9731500148773193\n",
      "Test set accuracy 0.9662999510765076\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for x, y in training_generator:\n",
    "    y = one_hot(y, n_targets)\n",
    "    params = update(params, x, y)\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  train_acc = accuracy(params, train_images, train_labels)\n",
    "  test_acc = accuracy(params, test_images, test_labels)\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
